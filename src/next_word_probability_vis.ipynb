{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "\n",
    "from utils import seed_all_randomness, load_corpus, str2bool\n",
    "import utils_testing\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from gpt2_model.tokenization_gpt2 import GPT2Tokenizer\n",
    "from gpt2_model.modeling_gpt2_multi import GPT2MultiLMHeadModel, GPT2MoSLMHeadModel, GPT2LMHeadModel, GPT2Model\n",
    "from gpt2_model.configuration_gpt2 import GPT2Config\n",
    "import argparse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch Train Future Topic Prediction')\n",
    "\n",
    "def add_model_options(parser, suffix):\n",
    "    parser.add_argument('--model_path'+suffix, type=str,  default='./models/',\n",
    "                        help='path to load the model')\n",
    "    parser.add_argument('--load_file_name'+suffix, type=str,  default='LM_weights.pt',\n",
    "                    help='file name of saved model')\n",
    "    parser.add_argument('--n_facet'+suffix, type=int, default=1,\n",
    "                        help='number of facets')\n",
    "    parser.add_argument('--n_facet_hidden'+suffix, type=int, default=1,\n",
    "                        help='number of hidden states')\n",
    "    parser.add_argument('--n_facet_MLP'+suffix, type=int, default=0,\n",
    "                        help='size of compression layer')\n",
    "    parser.add_argument('--n_facet_window'+suffix, type=int, default=0,\n",
    "                        help='size of windows we look at')\n",
    "    parser.add_argument('--n_facet_effective'+suffix, type=int, default=1,\n",
    "                        help='number of facet heads')\n",
    "    \n",
    "    parser.add_argument('--use_avg'+suffix, type=str2bool, nargs='?', default=False,\n",
    "                        help='Whether we want to add an average embedding term to stablize the training')\n",
    "    parser.add_argument('--use_MoS'+suffix, type=str2bool, nargs='?', default=True,\n",
    "                        help='Whether we want to do the normalization for each facet (i.e., use mixture of softmax)')\n",
    "    parser.add_argument('--weight_mode'+suffix, type=str,  default='dynamic',\n",
    "                        help='could be empty, dynamic, and statis')\n",
    "    parser.add_argument('--use_proj_bias'+suffix, type=str2bool, nargs='?', default=True,\n",
    "                        help='Whether we want to add an bias term in the linear projection layer')\n",
    "    parser.add_argument('--efficient_mode'+suffix, type=str,  default='None',\n",
    "                        help='how to save computational time')\n",
    "    parser.add_argument('--masking_ratio'+suffix, type=float, default=-1,\n",
    "                        help='dynamically use single facets. Use -1 to turn off this efficient mode')\n",
    "    parser.add_argument('--last_num'+suffix, type=int, default=0,\n",
    "                        help='number of facet that does not have multiple partitions')\n",
    "    \n",
    "    \n",
    "\n",
    "# parser.add_argument('--data', type=str, default='./data/processed/wiki2016_gpt2/',\n",
    "#                     help='location of the data corpus')\n",
    "# parser.add_argument('--tensor_folder', type=str, default='tensors_all_min100',\n",
    "#                     help='location of the data corpus')\n",
    "# parser.add_argument('--model_path_multi', type=str,  default='./models/',\n",
    "#                     help='path to load the multi-facet model')\n",
    "# parser.add_argument('--model_path_single', type=str,  default='./models/',\n",
    "#                     help='path to load the single-facet model')\n",
    "parser.add_argument('--outf', type=str, default='gen_log/generated.txt',\n",
    "                    help='output file for generated text')\n",
    "\n",
    "parser.add_argument('--batch_size', type=int, default=4, metavar='N',\n",
    "                    help='batch size')\n",
    "parser.add_argument('--bptt', type=int, default=256,\n",
    "                    help='sequence length')\n",
    "parser.add_argument('--max_batch_num', type=int, default=100,\n",
    "                    help='number of batches for evaluation')\n",
    "parser.add_argument('--num_sent_gen', type=int, default=3, metavar='N',\n",
    "                    help='In each prompt, generate how many sentences')\n",
    "parser.add_argument('--gen_sent_len', type=int, default=50, metavar='N',\n",
    "                    help='In each prompt, generate sentences with length gen_sent_len')\n",
    "\n",
    "parser.add_argument('--run_eval', type=str2bool, nargs='?', default=True,\n",
    "                    help='If false, we only print the results')\n",
    "\n",
    "parser.add_argument('--cuda', type=str2bool, nargs='?', default=True,\n",
    "                    help='use CUDA')\n",
    "parser.add_argument('--seed', type=int, default=1111,\n",
    "                    help='random seed')\n",
    "\n",
    "add_model_options(parser, \"_multi\")\n",
    "add_model_options(parser, \"_single\")\n",
    "\n",
    "\n",
    "def load_model(model_path, load_file_name, gpt2_config, n_facet, n_facet_window, n_facet_hidden, n_facet_MLP, n_facet_effective, weight_mode, use_avg, use_MoS, use_proj_bias, efficient_mode, last_num, device, cuda):\n",
    "    LM_state_dict = torch.load(os.path.join(model_path, load_file_name), map_location=device)\n",
    "    #GPT2_LM = GPT2MultiLMHeadModel.from_pretrained(model_name, state_dict = LM_state_dict)\n",
    "    GPT2_encoder = GPT2Model(gpt2_config)\n",
    "    if use_MoS:\n",
    "        #weight_mode = 'dynamic'\n",
    "        #weight_mode = 'static'\n",
    "        #weight_mode = ''\n",
    "        GPT2_LM = GPT2MoSLMHeadModel(gpt2_config, GPT2_encoder, n_facet, n_facet_hidden, weight_mode, use_proj_bias, \n",
    "                                     n_facet_window = n_facet_window, n_facet_MLP = n_facet_MLP, efficient_mode=efficient_mode, \n",
    "                                     device=device, n_facet_effective_in=n_facet_effective, last_num=last_num)\n",
    "    else:\n",
    "        GPT2_LM = GPT2MultiLMHeadModel(gpt2_config, GPT2_encoder, n_facet, n_facet_hidden, use_avg)\n",
    "    GPT2_LM.load_state_dict(LM_state_dict)\n",
    "    if args.cuda:\n",
    "        GPT2_LM = GPT2_LM.cuda()\n",
    "    return GPT2_LM\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args: Namespace(batch_size=4, bptt=256, cuda=True, efficient_mode_multi='even_last_2', efficient_mode_single='None', gen_sent_len=50, last_num_multi=0, last_num_single=0, load_file_name_multi='LM_weights_8.pt', load_file_name_single='LM_weights_8.pt', masking_ratio_multi=-1, masking_ratio_single=-1, max_batch_num=100, model_path_multi='../models/gpt2_wiki_n3_init-20210320-145419', model_path_single='../models/gpt2_wiki_n1_init-20210316-011244', n_facet_MLP_multi=-1, n_facet_MLP_single=-1, n_facet_effective_multi=3, n_facet_effective_single=1, n_facet_hidden_multi=3, n_facet_hidden_single=3, n_facet_multi=6, n_facet_single=1, n_facet_window_multi=-2, n_facet_window_single=-2, num_sent_gen=3, outf='gen_log/generated.txt', run_eval=True, seed=11, use_MoS_multi=True, use_MoS_single=True, use_avg_multi=False, use_avg_single=False, use_proj_bias_multi=True, use_proj_bias_single=True, weight_mode_multi='dynamic', weight_mode_single='dynamic')\n"
     ]
    }
   ],
   "source": [
    "#args_str_list = (\"--model_path_multi ./models/gpt2_wiki_n3_init-20210320-145419\").split()\n",
    "#args_str_list = (\"--model_path_multi ../models/gpt2_wiki_n3_init-20210218-133219 --load_file_name_multi LM_weights_8.pt --n_facet_multi 3 --n_facet_effective_multi 3 --n_facet_window_multi -2 --n_facet_hidden_multi 3 --n_facet_MLP_multi -1\" +\n",
    "\n",
    "args_str_list = (\"--model_path_multi ../models/gpt2_wiki_n3_init-20210320-145419 --load_file_name_multi LM_weights_8.pt --n_facet_multi 6 --n_facet_effective_multi 3 --n_facet_window_multi -2 --n_facet_hidden_multi 3 --n_facet_MLP_multi -1 --efficient_mode_multi even_last_2\" +\n",
    "                         \" --model_path_single ../models/gpt2_wiki_n1_init-20210316-011244 --load_file_name_single LM_weights_8.pt --n_facet_single 1 --n_facet_window_single -2 --n_facet_hidden_single 3 --n_facet_MLP_single -1\" +\n",
    "                          \" --seed 11 --cuda True\").split()\n",
    "\n",
    "#args_str_list = (\"--model_path_multi ../models/gpt2_wiki_n3_init-20210224-004334 --load_file_name_multi LM_weights_8.pt --n_facet_multi 3 --n_facet_effective_multi 3 --n_facet_window_multi -2 --n_facet_hidden_multi 3 --n_facet_MLP_multi -1\" +\n",
    "#                         \" --model_path_single ../models/gpt2_wiki_n1_init-20210224-004343 --load_file_name_single LM_weights_8.pt --n_facet_single 1 --n_facet_window_single -2 --n_facet_hidden_single 3 --n_facet_MLP_single -1\" +\n",
    "#                          \" --seed 11 --cuda True\").split()\n",
    "\n",
    "#model_name = 'gpt2-medium'\n",
    "model_name = 'gpt2'\n",
    "\n",
    "\n",
    "#print(args_str_list)\n",
    "args = parser.parse_args(args_str_list)\n",
    "\n",
    "\n",
    "# Set the random seed manually for reproducibility.\n",
    "seed_all_randomness(args.seed,args.cuda)\n",
    "\n",
    "print('Args: {}'.format(args))\n",
    "\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "\n",
    "gpt2_config = GPT2Config.from_pretrained(model_name)\n",
    "gpt2_config.output_hidden_states = True\n",
    "\n",
    "\n",
    "model_multi = load_model(args.model_path_multi, args.load_file_name_multi, gpt2_config, args.n_facet_multi, args.n_facet_window_multi, \n",
    "                         args.n_facet_hidden_multi, args.n_facet_MLP_multi, args.n_facet_effective_multi, \n",
    "                         args.weight_mode_multi, args.use_avg_multi, args.use_MoS_multi, args.use_proj_bias_multi, \n",
    "                         args.efficient_mode_multi, args.last_num_multi, device, args.cuda)\n",
    "\n",
    "model_single = load_model(args.model_path_single, args.load_file_name_single, gpt2_config, args.n_facet_single, args.n_facet_window_single, \n",
    "                         args.n_facet_hidden_single, args.n_facet_MLP_single, args.n_facet_effective_single, \n",
    "                         args.weight_mode_single, args.use_avg_single, args.use_MoS_single, args.use_proj_bias_single, \n",
    "                         args.efficient_mode_single, args.last_num_single, device, args.cuda)\n",
    "\n",
    "tokenizer_GPT2 = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "vocab_map = dict(tokenizer_GPT2.encoder, **tokenizer_GPT2.added_tokens_encoder)\n",
    "#print(vocab_map)\n",
    "#print(tokenizer_GPT2.encoder)\n",
    "vocab_size = len(vocab_map)\n",
    "idxl2token= ['']*vocab_size\n",
    "for w in vocab_map:\n",
    "    idx=vocab_map[w]\n",
    "    idxl2token[idx]=w\n",
    "#print(idxl2token)\n",
    "\n",
    "#idxl2token = tokenizer_GPT2.decode(torch.tensor(range()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_word(model, input_token):\n",
    "    top_k = 10\n",
    "    #temperature = 1\n",
    "    outputs, emb_div, count_best_arr, weight = model(input_token)#, labels=input_token)\n",
    "    #print(outputs)\n",
    "    probs = outputs[0]\n",
    "    probs = probs[:, -1, :] #/ temperature\n",
    "    #probs = F.softmax(logits, dim=-1)\n",
    "    probs_top, index_top = torch.topk(probs, k=top_k)\n",
    "    return probs_top, index_top\n",
    "\n",
    "def visualize_top_k_words(probs_top, index_top, idxl2token):\n",
    "    probs_top_list = probs_top.squeeze().tolist()\n",
    "    index_top_list = index_top.squeeze().tolist()\n",
    "    #print(index_top_list)\n",
    "    topk = len(probs_top_list)\n",
    "    for i in range(topk):\n",
    "        print(\"{}  {}  {}\".format(i,idxl2token[index_top_list[i]],probs_top_list[i]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are an uncle and a woman in front of me, and I talk to the\n",
      "tensor([[1318,  389,  281, 7711,  290,  257, 2415,  287, 2166,  286,  502,   11,\n",
      "          290,  314, 1561,  284,  262]], device='cuda:0')\n",
      "Multi-facet\n",
      "0  Ġuncle  0.14971046149730682\n",
      "1  Ġwoman  0.11480355262756348\n",
      "2  Ġman  0.08414516597986221\n",
      "3  Ġtwo  0.027981584891676903\n",
      "4  Ġother  0.027950972318649292\n",
      "5  Ġgirl  0.02295503579080105\n",
      "6  Ġmother  0.018764926120638847\n",
      "7  Ġboy  0.01686326414346695\n",
      "8  Ġlady  0.014466330409049988\n",
      "9  Ġaunt  0.014079896733164787\n",
      "\n",
      "Single-facet\n",
      "0  Ġwoman  0.14697295427322388\n",
      "1  Ġman  0.10963626205921173\n",
      "2  Ġother  0.032980743795633316\n",
      "3  Ġtwo  0.02860880084335804\n",
      "4  Ġgirl  0.02445436269044876\n",
      "5  Ġuncle  0.02117065340280533\n",
      "6  Ġmother  0.018148234114050865\n",
      "7  Ġlady  0.016263790428638458\n",
      "8  Ġboy  0.013309802860021591\n",
      "9  Ġpriest  0.011639346368610859\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tokenizer_GPT2.encode\n",
    "\n",
    "model_multi.eval()\n",
    "model_single.eval()\n",
    "\n",
    "#For medium\n",
    "#prompt = \"There are a queen and a man in front of me, and I talk to the\"\n",
    "#prompt = \"There are an uncle and a woman in front of me, and I talk to the\"\n",
    "#prompt = \"There are an aunt and a man in front of me, and I talk to the\"\n",
    "#prompt = \"There are a niece and a woman in front of me, and I talk to the\"\n",
    "#prompt = \"John, Kathryn and Mary are in their home, and John is watching TV with \"\n",
    "#prompt = \"John, Kathryn and Mary are in their home, and Mary is watching TV with \"\n",
    "\n",
    "#For small\n",
    "prompt = \"There are an uncle and a woman in front of me, and I talk to the\"\n",
    "#prompt = \"John, Jenny and Mary are in their home, and John is watching TV with \"\n",
    "#prompt = \"There are plates and balloons in front of me, and I pick up the\"\n",
    "\n",
    "#prompt = \"Obama plans to visit Beijing and Russian, and his flight first arrives at\"\n",
    "#prompt = \"There are a few children and a dog in front of me, and I choose to play with the\"\n",
    "#prompt = \"There are a few cats and a dog in front of me, and I choose to play with the\"\n",
    "#prompt = \"There are plates and a balloon in front of me, and I pick up the\"\n",
    "#prompt = \"There are balls and a balloon in front of me, and I pick up the\"\n",
    "#prompt = \"There are a king and a woman in front of me, and I talk to the\"\n",
    "#prompt = \"There are a policeman and a bride in front of me, and I talk to the\"\n",
    "#prompt = \"There are a prince and a woman in front of me, and I talk to the\"\n",
    "#prompt = \"There are a prince and women in front of me, and I talk to the\"\n",
    "#prompt = \"There are a man and a princess in front of me, and I talk to the\"\n",
    "#prompt = \"There are a prince and a bride in front of me, and I talk to the\"\n",
    "#prompt = \"There are a son and a woman in front of me, and I talk to the\"\n",
    "#prompt = \"There are a sister and a man in front of me, and I talk to the\"\n",
    "#prompt = \"There are the poor and the quiet in front of me, and I talk to the\"\n",
    "#prompt = \"There are the rich and the loud in front of me, and I talk to the\"\n",
    "\n",
    "\n",
    "#prompt = \"John and Mary are in their home, and a bullet hits\"\n",
    "#prompt = \"A king and a woman are in their home, and the\"\n",
    "#prompt = \"A king and a woman go to a park, and a dog attacks one of them, who is\"\n",
    "#prompt = \"Obama plans to visit Paris and England, and his flight first arrives at\"\n",
    "#prompt = \"I went to Paris and England before, and I love one of the places more, which is\"\n",
    "#prompt = \"I went to France and London before, and I love one of the places more, which is\"\n",
    "#prompt = \"I went to Boston and California before, and I love one of the places more, which is\"\n",
    "#prompt = \"I went to Paris and China before, and I love one of the places more, which is\"\n",
    "#prompt = \"Obama plans to visit Greece and Baghdad, and his flight first arrives at\"\n",
    "#prompt = \"I went to Greece and Baghdad before, and I love one of the places more, which is\"\n",
    "\n",
    "tokenized_text = tokenizer_GPT2.tokenize(prompt, add_prefix_space=True)\n",
    "indexed_tokens = tokenizer_GPT2.convert_tokens_to_ids(tokenized_text)\n",
    "indexed_tokens_tensor= torch.tensor(indexed_tokens, device=device, dtype=torch.long)\n",
    "indexed_tokens_tensor = indexed_tokens_tensor.unsqueeze(0)\n",
    "\n",
    "print(prompt)\n",
    "print(indexed_tokens_tensor)\n",
    "\n",
    "probs_top, index_top = generate_next_word(model_multi, indexed_tokens_tensor)\n",
    "print(\"Multi-facet\")\n",
    "visualize_top_k_words(probs_top, index_top, idxl2token)\n",
    "\n",
    "probs_top, index_top = generate_next_word(model_single, indexed_tokens_tensor)\n",
    "print(\"Single-facet\")\n",
    "visualize_top_k_words(probs_top, index_top, idxl2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
